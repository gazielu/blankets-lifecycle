{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20a3fb3a-fd2d-496d-8d7e-a069742b23b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Load txt to parquet -  ingestion phase\n",
    "\n",
    "1) Dataframe schema definition<BR> \n",
    "2) Read data from azure Row folder to landing zone<BR>\n",
    "3) Create Audit Log ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.Setup Folder and Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1 List files in relevent storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore') #, category=FutureWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\BI\\GitHub\\LearningSparkV2-master\\SuppliesML\\02-Data-Engineering\\pyspark\\02-LoadData\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18b65c62-9023-4010-b598-d3ad93445b3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "This is an example of how to list things you need to use the software and how to install them.\n",
    "* Azure blob storage indigo \n",
    "  ```sh\n",
    " display(dbutils.fs.ls(srcDataDirRoot))\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc1c703b-1769-4251-8979-eccb65f80f4d",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MOUNTPOINT = \"c:\\BI\\GitHub\\LearningSparkV2-master\\SuppliesML\"\n",
    "SourcePath = os.path.join(MOUNTPOINT, \"Data\",\"Dev\")\n",
    "RawPath    = os.path.join(SourcePath,\"Raw\")\n",
    "ResearchPath = os.path.join(SourcePath,\"Research\")\n",
    "LandingPath = os.path.join(SourcePath, \"Landing\")\n",
    "LoadingPath = os.path.join(SourcePath,  \"Loading\")\n",
    "certified_pit = os.path.join(SourcePath,\"certified-pit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3e57f21-22a5-4fd1-9205-b35507b0929f",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define source and destination directories\n",
    "srcDataDirRoot = RawPath #Root dir for source data\n",
    "destDataDirRoot = LandingPath #Root dir for consumable data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1db5b1e7-b1b3-460e-ab3f-cd14e2e4005a",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "source": [
    "### 1.2 Define Schema for the file that loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed56d839-dc8c-4677-8d9f-1093a0975a8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Required for StructField, StringType, IntegerType, etc.\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, FloatType, TimestampType\n",
    "\n",
    "\n",
    "# # Space bi inc- sensor data \n",
    "ProductionRunSchema = StructType([\n",
    "  StructField(\"Product\" , StringType() ,True),\n",
    "  StructField(\"Machine\" , StringType() ,True),\n",
    "  StructField(\"FOLDER_PATH\" , StringType() ,True),\n",
    "  StructField(\"Product_category\" , StringType() ,True),\n",
    "  StructField(\"Product_eng_name\" , StringType() ,True),\n",
    "  StructField(\"Series\" , StringType() ,True),\n",
    "  StructField(\"Product_name_win\" , StringType() ,True),\n",
    "  StructField(\"Size_Flag\" , StringType() ,True),\n",
    "  StructField(\"Parameter_Name\" , StringType() ,True),\n",
    "  StructField(\"Batch\" , StringType() ,True),\n",
    "  StructField(\"SAMPLE_ID\" , FloatType() ,True),\n",
    "  StructField(\"SAMPLE_Date\" , TimestampType() ,True),\n",
    "  StructField(\"SK_Sample_Date\" , IntegerType() ,True),\n",
    "  StructField(\"Parameter_Critical_Flag\" , StringType() ,True),\n",
    "  StructField(\"Is_Sample_Deleted_Flg\" , StringType() ,True),\n",
    "  StructField(\"SAMPLE_Mean\" , FloatType() ,True),\n",
    "  StructField(\"SAMPLE_stdev\" , FloatType() ,True),\n",
    "  StructField(\"SAMPLE_Minimum\" , FloatType() ,True),\n",
    "  StructField(\"SAMPLE_Maximum\" , FloatType() ,True),\n",
    "  StructField(\"SAMPLE_Median\" , FloatType() ,True),\n",
    "  StructField(\"Spec_target\" , FloatType() ,True),\n",
    "  StructField(\"SAMPLE_Size\" , FloatType() ,True),\n",
    "  StructField(\"LSL\" , FloatType() ,True),\n",
    "  StructField(\"USL\" , FloatType() ,True),\n",
    "  StructField(\"SL_enabled\" , StringType() ,True),\n",
    "  StructField(\"CH_ID\" , FloatType() ,True),\n",
    "  StructField(\"ETL_DATE\" , TimestampType() ,True)\n",
    "])\n",
    "\n",
    "\n",
    "# windigo production line\n",
    "New_Era_Run_Details_Schema = StructType([\n",
    "StructField(\"Run_Number\" , StringType() ,True),\n",
    "StructField(\"Blanket_Serial_Number\" , StringType() ,True),  \n",
    "StructField(\"Blanket_SEQ_NR\" , StringType() ,True),  \n",
    "StructField(\"Plant_Id\" , IntegerType() ,True),\n",
    "StructField(\"Quality_Status_Id\" , IntegerType() ,True),\n",
    "StructField(\"Body_ID\" , StringType() ,True),\n",
    "StructField(\"CSL_ID\" , StringType() ,True),\n",
    "StructField(\"BLK_Quality_Status_Name\" , StringType() ,True),\n",
    "StructField(\"BLK_Quality_Status_Flag\" , IntegerType() ,True),\n",
    "StructField(\"Blanket_Legacy_Part_Nr\" , StringType() ,True),\n",
    "StructField(\"Product_Engineering_Name\" , StringType() ,True),\n",
    "StructField(\"Source_System_Modified_DateTime\" , TimestampType() ,True)\n",
    "])\n",
    "\n",
    "\n",
    "# # Customer data - lifespan\n",
    "Blanket_lifespan_installed_base_Schema = StructType([\n",
    "#StructField(\"Fact_PIP_IMPACT_RowID\", IntegerType() ,True),\n",
    "StructField(\"Blanket_Impact_RowID\", StringType() ,True),\n",
    "StructField(\"Press_Serial_Number\", StringType() ,True),\n",
    "StructField(\"BLANKETS_ID\", StringType() ,True),\n",
    "StructField(\"Replacement_DateTime\", StringType() ,True),\n",
    "StructField(\"End_User_Code\", StringType() ,True),\n",
    "StructField(\"Domain\", StringType() ,True),\n",
    "StructField(\"ROR\", StringType() ,True),\n",
    "StructField(\"Consumable_Type\", StringType() ,True),\n",
    "StructField(\"Optimized_Lifespan\", IntegerType() ,True),\n",
    "StructField(\"Is_Last_Replacement\", StringType() ,True),\n",
    "StructField(\"Is_Lifespan_Official\", StringType() ,True),\n",
    "StructField(\"Consumable_Maturity\", StringType() ,True),\n",
    "StructField(\"DOA_Count\", IntegerType() ,True),\n",
    "StructField(\"DOP_Count\", IntegerType() ,True),\n",
    "StructField(\"RowID\", IntegerType() ,True),\n",
    "StructField(\"Changed_Date_Time\", StringType() ,True),\n",
    "StructField(\"Replacement_Monthly_Date_Id\", IntegerType() ,True),\n",
    "StructField(\"ETL_Date\", StringType() ,True),\n",
    "#StructField(\"Press_Classification\", StringType() ,True),\n",
    "#StructField(\"Lifespan_Guidelines\", DoubleType() ,True),\n",
    "StructField(\"Click_Charge\", StringType() ,True),\n",
    "StructField(\"Ownership\", StringType() ,True),\n",
    "StructField(\"Product_Number\", StringType() ,True),\n",
    "StructField(\"Description\", StringType() ,True),\n",
    "StructField(\"Product_Group\", StringType() ,True),\n",
    "StructField(\"Press_Group\", StringType() ,True),\n",
    "StructField(\"Family_type\", StringType() ,True),\n",
    "StructField(\"Series\", StringType() ,True),\n",
    "StructField(\"Press_Segment\", StringType() ,True),\n",
    "StructField(\"Current_SW_Version_ID\", StringType() ,True),\n",
    "StructField(\"Customer_Name\", StringType() ,True),\n",
    "StructField(\"Site_Region\", StringType() ,True),\n",
    "StructField(\"Site_Sub_Region\", StringType() ,True),\n",
    "StructField(\"Site_Country\", StringType() ,True),\n",
    "#StructField(\"ETL_Date\" , TimestampType() ,True)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.Create Spark Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 check warehouse Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\BI\\GitHub\\LearningSparkV2-master\\SuppliesML\\supplies-warehouse\n"
     ]
    }
   ],
   "source": [
    "from os.path import abspath\n",
    "import pyodbc\n",
    "warehouse_location = abspath('..\\..\\..\\supplies-warehouse')\n",
    "print(warehouse_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import abspath\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "# warehouse_location points to the default location for managed databases and tables\n",
    "#warehouse_location = abspath('..\\..\\..\\supplies-warehouse')\n",
    "warehouse_location = abspath('..\\..\\..\\hive_supplies-warehouse')\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\")\\\n",
    ".config(\"spark.hadoop.hive.metastore.warehouse.dir\", warehouse_location)\\\n",
    ".config('spark.driver.extraClassPath', 'C:\\Program Files\\Microsoft JDBC Driver 10.2 for SQL Server\\sqljdbc_10.2\\enu\\mssql-jdbc-10.2.1.jre8.jar') \\\n",
    ".config('spark.executor.extraClassPath', 'C:\\Program Files\\Microsoft JDBC Driver 10.2 for SQL Server\\sqljdbc_10.2\\enu\\mssql-jdbc-10.2.1.jre8.jar') \\\n",
    ".config(\"spark.sql.catalogImplementation\",\"hive\") \\\n",
    ".enableHiveSupport() \\\n",
    ".getOrCreate() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. set data source Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Set odbc Connections > user + Password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Driver={SQL Server Native Client 11.0};Server=myServerAddress;Database=myDataBase;Uid=myUsername;Pwd=myPassword;\n",
    "\n",
    "sql_native_space = pyodbc.connect(\"Driver={SQL Server Native Client 11.0};\"\n",
    "                        \"Server=gvs72076.inc.hpicorp.net,2048;\"\n",
    "                        \"Database=faomrp;\"\n",
    "                        \"Uid=SPACE_BI_IDS;\"\n",
    "                        \"Pwd=SPC-IDS!pswd;\")\n",
    "                  \n",
    "# > windows trusted authication  - when we user password please assign it to \"no\"\n",
    "# conn_GABI.close()\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyodbc.Connection object at 0x000001F6E8EBA800> faomrp\n",
      "<pyodbc.Connection object at 0x000001F6E8EBA800> GABI-P\n"
     ]
    },
    {
     "ename": "ProgrammingError",
     "evalue": "('42000', \"[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Login failed for user 'Tableau_Itg'. Reason: The account is disabled. (18470) (SQLDriverConnect); [42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Login failed for user 'Tableau_Itg'. Reason: The account is disabled. (18470)\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [36], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m username \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mTableau_Itg\u001b[39m\u001b[39m'\u001b[39m \n\u001b[0;32m     26\u001b[0m password \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mTab_itg4\u001b[39m\u001b[39m'\u001b[39m \n\u001b[1;32m---> 27\u001b[0m odbc_Hubble_ITG \u001b[39m=\u001b[39m pyodbc\u001b[39m.\u001b[39;49mconnect(\u001b[39m'\u001b[39;49m\u001b[39mDRIVER=\u001b[39;49m\u001b[39m{\u001b[39;49m\u001b[39mODBC Driver 17 for SQL Server};SERVER=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49mserver\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m;DATABASE=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49mdatabase\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m;UID=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49musername\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m;PWD=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49m password)\n\u001b[0;32m     28\u001b[0m \u001b[39mprint\u001b[39m(odbc_Hubble_ITG,database)\n",
      "\u001b[1;31mProgrammingError\u001b[0m: ('42000', \"[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Login failed for user 'Tableau_Itg'. Reason: The account is disabled. (18470) (SQLDriverConnect); [42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Login failed for user 'Tableau_Itg'. Reason: The account is disabled. (18470)\")"
     ]
    }
   ],
   "source": [
    "server = 'gvs72076.inc.hpicorp.net,2048' \n",
    "database = 'faomrp' \n",
    "username = 'SPACE_BI_IDS' \n",
    "password = 'SPC-IDS!pswd' \n",
    "odbc_SPACE_BI = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "print(odbc_SPACE_BI,database)\n",
    "\n",
    "# server = '106775-2s-ag1.corp.hpicloud.net,2048' \n",
    "# database = 'GABI-I' \n",
    "# username = 'GABI_STG_RW_1' \n",
    "# password = 'change_pw_first_login2020$' \n",
    "# odbc_SPACE_BI = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "# print(odbc_SPACE_BI,database)\n",
    "\n",
    "server = 'gvs72069.inc.hpicorp.net,2048' \n",
    "database = 'GABI-P' \n",
    "username = 'GABI_STG_RW_1' \n",
    "password = 'fT7_E*4U6k_m' \n",
    "odbc_GABI_P = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "print(odbc_SPACE_BI,database)\n",
    "\n",
    "\n",
    "server = 'WIN-OA97GH5SPHI.CORP.HPICLOUD.NET,1433' # 172.20.206.100\n",
    "database = 'Hubble_DWH_ITG' \n",
    "username = 'Tableau_Itg' \n",
    "password = 'Tab_itg4' \n",
    "odbc_Hubble_ITG = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "print(odbc_Hubble_ITG,database)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Set odbc Connections > user authntication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyodbc.Connection object at 0x000001F6EA9169A0>\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "Driver=\"{ODBC Driver 17 for SQL Server}\"\n",
    "server=\"172.20.206.100,1433\"\n",
    "database=\"HUBBLE_DWH_ITG\"\n",
    "trusted_connection=\"yes\"\n",
    "odbc_Hubble_ITG = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';Database='+database+';trusted_connection='+trusted_connection)\n",
    "print(odbc_Hubble_ITG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Driver={SQL Server Native Client 11.0};Server=myServerAddress;Database=myDataBase;Uid=myUsername;Pwd=myPassword;\n",
    "import pyodbc\n",
    "sql_native_Hubble_ITG = pyodbc.connect(\"Driver={SQL Server Native Client 11.0};\"\n",
    "                        \"Server=172.20.206.100,1433;\"\n",
    "                        \"Database=HUBBLE_DWH_ITG;\"\n",
    "                        \"Uid=Tableau_Itg;\"\n",
    "                        \"Pwd=Tab_itg4;\")\n",
    "                  \n",
    "# > windows trusted authication  - when we user password please assign it to \"no\"\n",
    "# conn_GABI.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Set odbc Connections > user + passward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqlsUrl = 'jdbc:sqlserver://172.20.206.100:1433;encrypt=true;databaseName=HUBBLE_DWH_ITG;integratedSecurity=true;trustServerCertificate=true;'\n",
    "\n",
    ">> there is no integrated security "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 set jdbc Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sqlsUrl = 'jdbc:sqlserver://gvs72069.inc.hpicorp.net:2048;encrypt=true;database=GABI-P;user=GABI_STG_RW_1;password=fT7_E*4U6k_m;trustServerCertificate=true;'\n",
    "JdbcsqlsUrl_space = 'jdbc:sqlserver://gvs72076.inc.hpicorp.net:2048;encrypt=true;database=faomrp;user=SPACE_BI_IDS;password=SPC-IDS!pswd;trustServerCertificate=true;'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JdbcsqlsUrl_NewEra = 'jdbc:sqlserver://gvs72069.inc.hpicorp.net:2048;encrypt=true;database=GABI-P;user=GABI_STG_RW_1;password=fT7_E*4U6k_m;trustServerCertificate=true;'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JdbcsqlsUrl_blanketslifespan = 'jdbc:sqlserver://WIN-OA97GH5SPHI.CORP.HPICLOUD.NET:1433;encrypt=true;database=HUBBLE_DWH_ITG;user=Tableau_Itg;password=Tab_itg4;trustServerCertificate=true;'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. List of queires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Load with JDBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 5.1.1 Load from New Era"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time in seconds for: Table NewEra > YearMonth: 202301  is: 171.99   numberofrow:  75404\n",
      "total Execution time in seconds for: Table NewEra > all yearmonth is: 189.48\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta,date\n",
    "# Build Dynamic Query per yearmonth filter\n",
    "def Loaddata(YearMonth,StartYear,querypath):\n",
    "    \n",
    "    #YearMonth =\"201903\"\n",
    "    Year=YearMonth[0:4]\n",
    "    with open(querypath) as f: query_file = f.read()\n",
    "    sql_string = query_file.replace(\"{YearMonth}\",str(YearMonth)).replace(\"{StartYear}\",str(StartYear))\n",
    "    dynamic_sql = f\"\"\"{sql_string}\"\"\"\n",
    "    return dynamic_sql\n",
    "\n",
    "\n",
    "# Read Query from Source >  Return DATAFRAME per yearmonth filter\n",
    "def sprak_read_df(Query,schema):\n",
    "    \n",
    "    startTime = time.time()\n",
    "    spark_read_df = spark.read.format('jdbc') \\\n",
    "    .option('url',JdbcsqlsUrl_NewEra) \\\n",
    "    .option('driver', 'com.microsoft.sqlserver.jdbc.SQLServerDriver')\\\n",
    "    .option('query', Query) \\\n",
    "    .schema(schema) \\\n",
    "    .load()\n",
    "    executionTime = (time.time() - startTime)\n",
    "    #print('Execution time in seconds: ' + str(round(executionTime,2)))\n",
    "    return spark_read_df\n",
    "\n",
    "\n",
    "def WriteToParqurtFile(Dataframe,Zone,Table_Folder,Year,YearMonth):\n",
    "    Dataframe.coalesce(8).write.parquet(LandingPath + Table_Folder +str(Year)+\"/\"+str(YearMonth),mode=\"overwrite\") \n",
    "    executionTime = (time.time() - startTime)\n",
    "\n",
    "min_query_date= '2020-11-01'  \n",
    "StartYear = \"2018\"\n",
    "Zone = LandingPath\n",
    "Table_Folder = \"/New_Era/New_era_details/\"\n",
    "Table = \"NewEra\"\n",
    "QueryPath = \"IngestQueries/NewEra.sql\"\n",
    "startPipelineTime =  time.time()\n",
    "\n",
    "#for beg in pd.date_range(min_query_date,date.today().strftime(\"%Y-%m-01\"), freq='MS'): # date.today() date.today().strftime(\"%Y-%m-01\")\n",
    "for beg in pd.date_range('2023-01-01','2023-01-31', freq='MS'): # date.today()\n",
    "    startTime = time.time()\n",
    "    Yearmonth = beg.strftime(\"%Y%m\")\n",
    "    Year=Yearmonth[0:4]                                     \n",
    "    dynamic_sql = Loaddata(Yearmonth,StartYear,QueryPath)   \n",
    "    \n",
    "    Dataframe = sprak_read_df(dynamic_sql,New_Era_Run_Details_Schema)\n",
    "    WriteToParqurtFile(Dataframe,Zone,Table_Folder,Year,Yearmonth)\n",
    "    \n",
    "    executionTime = (time.time() - startTime)\n",
    "    print(f'Execution time in seconds for: Table '+Table+' > YearMonth: '+ Yearmonth +\"  is: \"+ str(round(executionTime,2))+ \"   numberofrow:  \" + str(Dataframe.count()))\n",
    "executionTime = (time.time() - startPipelineTime)\n",
    "print(f'total Execution time in seconds for: Table '+Table+' > all yearmonth is: '+ str(round(executionTime,2)))   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2.1 read & write & Test Count / Show() all NEW ERA yearmonth file to one parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_era_details_dfs = spark.read.format(\"parquet\").load(LandingPath + \"/New_Era/New_era_details/*/*\") # try to read empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time in seconds: 11.08\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "startTime = time.time()\n",
    "New_era_details_dfs.coalesce(1).write.parquet(LandingPath + \"/New_Era/New_era_history\",mode=\"overwrite\") #e\")#.saveAsTable(\"ld_blanket_lifespan_run_extended\")# = spark.read.format(\"parquet\").load(SourcePath + '/test')\n",
    "executionTime = (time.time() - startTime)\n",
    "print('Execution time in seconds: ' + str(round(executionTime,2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 5.1.2 Load from SpaceBI > production run jdbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "os.chdir(r'C:\\BI\\GitHub\\LearningSparkV2-master\\SuppliesML\\02-Data-Engineering\\pyspark\\02-LoadData')\n",
    "pwd()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"IngestQueries\\ProductionLine.sql\") as f: query_file = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "with Folders_Id_list AS (\n",
      "--******\n",
      "\n",
      "-- Create a table that contain all the relevent Blankets products for the ML analysis\n",
      "select  6743\t AS [FOLDER_ID],'New Era+AFL+WSM'     as [Folder Name], 0 AS Parent, 'BLANKETS' AS product, 8 As LDS_ID UNION\n",
      "select  9908     AS [FOLDER_ID],'New Era+AFL+WSM-> Gemini 3'     as [Folder Name],  1 AS Parent, 'GEMINI3' AS product, 8 As LDS_ID UNION\n",
      "select  9907     AS [FOLDER_ID],'New Era+AFL+WSM-> Timna 2'     as [Folder Name],  1 AS Parent, 'TIMNA 2' AS product, 8 As LDS_ID UNION\n",
      "select  11163     AS [FOLDER_ID],'New Era+AFL+WSM-> NewEra-SpeedUp'     as [Folder Name],  0 AS Parent, 'BLANKETS' AS product, 8 As LDS_ID UNION\n",
      "select  11165     AS [FOLDER_ID],'New Era+AFL+WSM-> NewEra-SpeedUp-> Iris Plus SP'     as [Folder Name],  1 AS Parent, 'IRIS PLUS_SP' AS product, 8 As LDS_ID UNION\n",
      "select  11168     AS [FOLDER_ID],'New Era+AFL+WSM-> NewEra-SpeedUp-> Rotem SP'     as [Folder Name],  1 AS Parent, 'ROTEM_SP' AS product, 8 As LDS_ID \n",
      ")\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      ",lds_ids AS(\n",
      "     select f.lds_id,\n",
      "\t         f.LDS_NAME,\n",
      "\t         sel.IS_SYSDATE,\n",
      "\t\t\t sel.NUMBER_OF_DAYS,\n",
      "\t\t\t case when sel.FROM_DT is null then  DATEADD(year, -8, getdate()) ELSE sel.FROM_DT END AS FROM_DT,\n",
      "\t\t\t case when sel.TO_DT is null then  DATEADD(day, 1, getdate()) ELSE sel.TO_DT END AS TO_DT\t\t\n",
      "      from [SPACE_BI_IDS].[T_LDS_HEADER] f\n",
      "\t  inner JOIN  [SPACE_BI_IDS].[SELECT_PARAM_FOR_VIEWS] sel on sel.VIEW_NAME='BLK'\n",
      "\t  where \n",
      "\t  CHARINDEX( ','+cast(f.lds_id as varchar) +',' ,sel.LDS_LST) > 0 \n",
      "\t  and f.lds_id in  (select LDS_ID From Folders_Id_list) -- uri gaziel\n",
      "\t  )\n",
      "\n",
      "\n",
      "  ,cte as (\n",
      "   select lds_ids.lds_id\n",
      "       , lds_ids.LDS_NAME\n",
      "       , C.FOLDER_ID\n",
      "\t   , C.[PARENT_FOLDER_ID]\n",
      "\t   , cast(C.[FOLDER_NAME] as varchar(max)) as FullName\n",
      "\t   , 0 as Rec_Level\n",
      "   from  [SPACE_BI_IDS].[T_FOLDER] C\n",
      "   inner join lds_ids on LDS_ID=C.ROOT_OBJECT_ID\n",
      "   where C.PARENT_FOLDER_ID=-1 \n",
      "   AND  C.FOLDER_ID in (select [FOLDER_ID] from Folders_Id_list)  -- uri gaziel\n",
      "  -- Recursive\n",
      "   UNION ALL\n",
      "   select c.lds_id\n",
      "          , c.LDS_NAME\n",
      "          ,c1.FOLDER_ID\n",
      "\t\t  , C1.PARENT_FOLDER_ID\n",
      "\t\t  , C.FullName  + '-> ' + C1.Folder_Name as FullName\n",
      "\t\t  , C.Rec_Level + 1 as Rec_Level\n",
      "   from [SPACE_BI_IDS].[T_FOLDER]  C1\n",
      "   inner join cte C on c1.Parent_folder_ID = c.FOLDER_ID and c1.PARENT_FOLDER_ID <> -1 \n",
      "   where c1.FOLDER_ID in (select [FOLDER_ID] from Folders_Id_list) \n",
      "   \n",
      "   --AND c1.FOLDER_ID in (6743,11165,11168)      -- uri gaziel \n",
      " ) \n",
      "--  as list\n",
      "--  where FOLDER_ID  in (6743,11165,11168))\n",
      "\n",
      "\n",
      " , lds_chs as (\n",
      "  select lds_ids.LDS_ID,\n",
      "  cte.LDS_NAME,\n",
      "  f.ch_id,\n",
      "  cte.FullName as FOLDER_PATH,\n",
      "    f.CH_NAME as CHANNEL_NM,\n",
      "   case when f.[CF_VALUE_01] is not null and f.[CF_VALUE_01]  ='Critical' then 'Y' else 'N' end Parameter_Critical_Flag\n",
      "   --case when f.[CF_VALUE_06] is not null and f.[CF_VALUE_06]  ='Yes' then 'Y' else 'N' end Parameter_PPK_Flag,\n",
      "  -- case when f.CF_VALUE_07 is not null AND f.CF_VALUE_07  ='Yes' then 'Y' else 'N' end Parameter_PPK_In_Spec,\n",
      "   --case when f.CF_VALUE_08 is not null AND f.CF_VALUE_08  ='Yes' then 'Y' else 'N' end Parameter_PPK_In_Control\n",
      "  from lds_ids\n",
      "  inner join [SPACE_BI_IDS].[T_CHANNEL_DEF] f on f.LDS_ID=lds_ids.LDS_ID\n",
      "  inner join cte on cte.LDS_ID=cte.LDS_ID and cte.FOLDER_ID=f.FOLDER_ID\n",
      "  where cte.FullName = 'New Era+AFL+WSM-> Timna 2' \n",
      "  --where cte.FOLDER_ID in (select [FOLDER_ID] from Folders_Id_list where parent =1)\n",
      " )\n",
      "\n",
      "\n",
      " ,lds_chs_key as (\n",
      "select distinct ch_id from lds_chs)\n",
      "\n",
      "\n",
      "--select * from lds_chs\n",
      ", External_sample AS (\n",
      "SELECT \n",
      "       [SAMPLE_ID] \n",
      "      ,[PARAMETER_NAME]\n",
      "      ,s.[LDS_ID]\n",
      "      ,SAMPLE_DATE   AS [SAMPLE_DATE]     \n",
      "      ,[EXT_SAMPLE_SIZE]\n",
      "      ,[EXT_SIGMA]\n",
      "      ,[EXT_MV]\n",
      "      ,[EXT_MIN]\n",
      "      ,[EXT_MAX]\n",
      "      ,[EXT_MEDIAN]\n",
      "      ,[SPEC_LOWER]\n",
      "      ,[SPEC_TARGET]\n",
      "      ,[SPEC_UPPER]\n",
      "      ,[EXVAL_01]\n",
      "      ,[EXVAL_03]\n",
      "      ,[EXVAL_06]  \n",
      "\t  ,[EXVAL_13]\n",
      "\t   ,case when (s.exval_01 ='NA' or s.exval_01 is null) and s.exval_13 <> 'NA'and s.exval_13\n",
      "\t\t  is not null then s.exval_13 else s.exval_01 end         Batch \n",
      "\t  ,SPC_FLAG_EXTERN\n",
      "\t  ,SPC_FLAG_INTERN\n",
      "      ,[SPEC_LIMIT_ENABLE] \n",
      "\t  ,sel.LDS_NAME\n",
      "\t  ,ETL_DM As ETL_DATE\n",
      "\t  --,sel.FOLDER_PATH AS [FOLDER PATH]\n",
      "  FROM [faomrp].[SPACE_BI_IDS].[T_EXT_SAMPLES] s\n",
      "  left join lds_ids sel on sel.[LDS_ID] = s.[LDS_ID]\n",
      "  Where  [EXVAL_03] in (select product from Folders_Id_list)\n",
      "  \n",
      "  \n",
      "  \n",
      "  --AND Year([SAMPLE_DATE])>{StartYear}\n",
      "  AND  SAMPLE_DATE >= '2022-12-01 00:00:00' AND SAMPLE_DATE <  '2023-01-01 00:00:00'\n",
      "  \n",
      "  --year(SAMPLE_DATE)*100+month(SAMPLE_DATE) = {YearMonth}\n",
      "  \n",
      "\tAND\n",
      "\t(\n",
      "\t(sel.IS_SYSDATE='Y' AND \n",
      "\ts.SAMPLE_DATE>= (GETDATE()-sel.NUMBER_OF_DAYS) AND s.SAMPLE_DATE <= GETDATE())  -- GETDATE() instead of SYSDATE\n",
      "\tOR\n",
      "\t(\n",
      "\t  sel.IS_SYSDATE='N' AND sel.FROM_DT is NOT NULL and sel.TO_DT is not null and\n",
      "\ts.SAMPLE_DATE >= sel.FROM_DT and s.SAMPLE_DATE <= sel.TO_DT \n",
      "\t)))\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "  ,External_sample_all AS (\n",
      "  select s.*, sc.[CH_ID],sc.[CKC_ID] from External_sample s\n",
      "  left join (\n",
      "SELECT  [SAMPLE_ID]\n",
      "      ,[CH_ID]\n",
      "      ,[CKC_ID]\n",
      "      --,[SAMPLE_DATE]  \n",
      "  FROM [faomrp].[SPACE_BI_IDS].[T_EXT_SAMPLES_CALC]\n",
      "  where --year(SAMPLE_DATE)*100+month(SAMPLE_DATE) = {YearMonth} \n",
      "   [CH_ID] in (select ch_id from lds_chs_key)) sc \n",
      "  on s.[SAMPLE_ID] = sc.SAMPLE_ID)\n",
      "\n",
      "  \n",
      " \n",
      "--select * from External_sample_all\n",
      "   --Create a list of channels for a period which is defined in the lds_ids and \n",
      "   --find out MAX(CKC_ID) in order to exclude records of t_ext_samples_calc with CKC_ID=0 in cases when channelshave the CKC_ID =0 and CKC_ID > 0 in its samples\n",
      "   --In additional START and END dates of the selection period per channel are defined based on sample_dates of t_ext_samples table \n",
      "   --(sample_dates of the t_ext_samples_calx table should not be used)\n",
      "   --Source tables are T_EXT_SAMPLES_CALC,T_EXT_SAMPLES, lds_chs,lds_ids (lds_chs will be included into this new table ch) for ch*/\n",
      "--select ch_id,SAMPLE_ID from [SPACE_BI_IDS].[T_EXT_SAMPLES_CALC] where CH_ID in (select * from lds_chs_key)\n",
      "\n",
      ",series_product AS\n",
      "(select *,\n",
      "Case when Product_category_win LIKE '%Gem%' or\n",
      "          Product_category_win LIKE '%Tim%' or\n",
      "\t\t  Product_category_win LIKE '%Kesh%' or\n",
      "          Product_category_win LIKE '%Hetz%' or\n",
      "\t\t  Product_category_win LIKE '%K2%'\n",
      "          then 'SF'\n",
      "\t\t  When\n",
      "\t\t  Product_category_win LIKE '%Iris%' or\n",
      "          Product_category_win LIKE '%Rotem%' or\n",
      "\t\t  Product_category_win LIKE '%O3%' or\n",
      "          Product_category_win LIKE '%Polaris%'\t\t  \n",
      "\t\t  then 'Web' end as      [SF / Web Flag]\n",
      "\n",
      ",Case when [PRT_DN] = 'GEMINI 3 BLANKET AUTO' Then   'GEMINI3'\n",
      "      when [PRT_DN] = 'TIMNA2 BLANKET REV 6 NE' Then 'TIMNA 2'\n",
      "\t  when [PRT_DN] = 'IRIS PLUS AFL PRE-CUT' Then   'IRIS PLUS_SP'\n",
      "\t  when [PRT_DN] = 'ROTEM BLANKET INITIAL CU' Then 'ROTEM_SP' Else [PRT_DN] End as [product Key]\n",
      "from (\n",
      "SELECT [PRT_DN]\n",
      "\t\t  ,max([PRDCT_CTGR_NM]) as Product_category_win\n",
      "\t      ,max([PRDCT_PRS_NM]) as Product_eng_name_win\n",
      "\t      ,max([PRDCT_SERS_NM]) as Series_win\n",
      "\t\t  ,max([PRDCT_NM]) as Product_name_win\n",
      "from [SPACE_BI_IDS].[WIND_PRT_PRDCT]\n",
      "where PLNT_ID=10 and PRDCT_CTGR_NM is not null\n",
      "group by [PRT_DN]) a)\n",
      "\n",
      ",ch as (\n",
      "  SELECT \n",
      "  --S.*, \n",
      "         --s.exval_03 as space_product,\n",
      "         c.lds_id, \n",
      "\t\t     c.ch_id,  \n",
      "         lds_chs.CHANNEL_NM,\n",
      "         max(CKC_ID) as MAX_CKC_ID,\n",
      "         MIN(c.SAMPLE_DATE) as MIN_SMPL_DT,\n",
      "         MAX(c.SAMPLE_DATE) as MAX_SMPL_DT,\n",
      "\t\t \n",
      "        lds_chs.FOLDER_PATH,\n",
      "\t\t\n",
      "\t\tlds_chs.Parameter_Critical_Flag\n",
      "\t\t/*\n",
      "\t\tlds_chs.Parameter_PPK_Flag,\n",
      "\t\tlds_chs.Parameter_PPK_In_Spec,\n",
      "\t\tlds_chs.Parameter_PPK_In_Control\n",
      "\t\t*/\n",
      "\t\t\n",
      " from   External_sample_all c\n",
      " left JOIN  lds_chs on lds_chs.CH_ID=c.CH_ID\n",
      " --inner join [SPACE_BI_IDS].[T_EXT_SAMPLES] s on s.SAMPLE_ID=c.SAMPLE_ID\n",
      " --inner join lds_ids sel on sel.LDS_ID=s.LDS_ID\n",
      " \n",
      " /*\n",
      " WHERE \n",
      "\t(\n",
      "\t(sel.IS_SYSDATE='Y' AND \n",
      "\ts.SAMPLE_DATE>= (GETDATE()-sel.NUMBER_OF_DAYS) AND s.SAMPLE_DATE <= GETDATE())  -- GETDATE() instead of SYSDATE\n",
      "\tOR\n",
      "\t(\n",
      "\t  sel.IS_SYSDATE='N' AND sel.FROM_DT is NOT NULL and sel.TO_DT is not null and\n",
      "\ts.SAMPLE_DATE >= sel.FROM_DT and s.SAMPLE_DATE <= sel.TO_DT \n",
      "\t)\n",
      "\t)\n",
      "\t*/\n",
      " group by c.lds_id,c.ch_id,lds_chs.CHANNEL_NM,lds_chs.FOLDER_PATH,lds_chs.Parameter_Critical_Flag--,lds_chs.Parameter_PPK_Flag,\n",
      " --lds_chs.Parameter_PPK_In_Spec,\tlds_chs.Parameter_PPK_In_Control--,Product_category_win,Series_win,Product_eng_name_win,Product_name_win,[Wide / Narrow Flag],[SF / Web Flag]\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",lds_dts AS\n",
      "(SELECT  \n",
      "   ch.LDS_ID,\n",
      "   lds_ids.LDS_NAME,\n",
      "   MIN(ch.MIN_SMPL_DT) as START_PRD_DT,\n",
      "   MAX(ch.MAX_SMPL_DT) as END_PRD_DT\n",
      "  from ch\n",
      "  inner join lds_ids on lds_ids.LDS_ID=ch.LDS_ID\n",
      "  group by ch.LDS_ID, lds_ids.LDS_NAME\n",
      "  )\n",
      "  /*\n",
      " select * from ch\n",
      "  Create a final list of lds-s with start and end periods per lds \n",
      "  Source tables are ch and lds_ids (lds_ids will be included into this new table lds_dts) for lds_dts\n",
      "*/\n",
      "\n",
      "\n",
      ",T_EXT_SAMPLES as (\n",
      "  SELECT \n",
      "   [SAMPLE_ID]\n",
      "  ,[PARAMETER_NAME]\n",
      "  ,c.[LDS_ID]\n",
      "  ,c.CH_ID\n",
      "  ,[SAMPLE_DATE]         \n",
      "  ,[EXT_SAMPLE_SIZE]\n",
      "  ,[EXT_SIGMA]\n",
      "  ,[EXT_MV]\n",
      "  ,[EXT_MIN]\n",
      "  ,[EXT_MAX]\n",
      "  ,[EXT_MEDIAN]\n",
      "  ,[SPEC_LOWER]\n",
      "  ,[SPEC_TARGET]\n",
      "  ,[SPEC_UPPER]\n",
      "\n",
      "  ,Batch\n",
      " -- ,[EXVAL_01]\n",
      "  ,[EXVAL_03]\n",
      "  ,[EXVAL_06]  \n",
      "--   ,[EXVAL_13]  \n",
      "  ,[SPEC_LIMIT_ENABLE]\n",
      "  ,SPC_FLAG_EXTERN\n",
      "  ,SPC_FLAG_INTERN,\n",
      "  --,sel.LDS_NAME,\n",
      "  c.CKC_ID,\n",
      "  lds_dts.START_PRD_DT,\n",
      "  lds_dts.END_PRD_DT,\n",
      "\n",
      "  --c.lds_name               LDS_Name,\n",
      "  ch.CHANNEL_NM,\n",
      "  ch.Parameter_Critical_Flag,\n",
      "  --ch.Parameter_PPK_In_Spec,\n",
      "  --ch.Parameter_PPK_In_Control,\n",
      "  --ch.Parameter_PPK_Flag,\n",
      "  lds_chs.FOLDER_PATH, \n",
      "  ch.MAX_CKC_ID,\n",
      "  \n",
      "  --Product_name_win,\n",
      "  Product_category_win,\n",
      "  Product_eng_name_win,\n",
      "  Series_win,\n",
      "  Product_name_win,\n",
      "  --Case when (Product_category_win LIKE '%Gem%' or\n",
      "  --        Product_category_win LIKE '%Iris%') and\n",
      "\t\t--  (cast(right(Batch,2) AS INT) >= 50 ) then 'Wide'\n",
      "\t\t--  WHEN \n",
      "\t\t--  (Product_category_win LIKE '%Gem%' or\n",
      "  --        Product_category_win LIKE '%Iris%') and\n",
      "\t\t--  (cast(right(Batch,2) AS INT) < 50 ) then 'Narrow'\n",
      "\t\t--  WHEN \n",
      "  --        Product_category_win LIKE '%Tim%' or\n",
      "\t\t--  Product_category_win LIKE '%Kesh%' or\n",
      "\t\t--  Product_category_win LIKE '%Rotem%' \t\t  \n",
      "\t\t--  then 'Wide'\n",
      "\t\t--  WHEN\n",
      "  --        Product_category_win LIKE '%Hetz%' or\t \n",
      "\t\t--  Product_category_win LIKE '%Polaris%' \n",
      "  --        then 'Naroow'\n",
      "\t\t--  end as      [Wide / Narrow Flag],\n",
      "  [SF / Web Flag],\n",
      "  --[Wide / Narrow Flag],\n",
      "  ETL_DATE\n",
      " \n",
      "  \n",
      "  from    [External_sample_all] c  \n",
      "  INNER JOIN  lds_chs on lds_chs.CH_ID=c.CH_ID\n",
      " inner join ch on ch.ch_id=c.ch_id \n",
      " inner join lds_dts on lds_dts.LDS_ID = c.LDS_ID\n",
      " left outer join series_product  on [product Key]= c.exval_03 \n",
      " where  len(Batch)=7 And right(left(Batch,7),2)<>'3M'\n",
      " )\n",
      "\n",
      "SELECT   \n",
      "\t      s.exval_03                                              Product --SAMPLE_PRODUCT\n",
      "\t\t  ,s.exval_06                                             Machine\n",
      "\t\t  ,FOLDER_PATH\n",
      "\t\t  ,Product_category_win                              AS  [Product_category]\n",
      "          ,Product_eng_name_win                              AS  [Product_eng_name]\n",
      "          ,Series_win                                        AS  [Series]\n",
      "          ,Product_name_win\n",
      "\t\t--  ,[Wide / Narrow Flag]   \n",
      "\t\t  ,[SF / Web Flag] as [size_Flag]\n",
      "     --     ,[Wide / Narrow Flag]\n",
      "\t\t  ,case when s.PARAMETER_NAME = s.CHANNEL_NM then\n",
      "\t\t  s.PARAMETER_NAME else s.CHANNEL_NM end                  Parameter_Name\n",
      "\t\t  ,Batch \n",
      "\t\t  ,s.sample_id                                            SAMPLE_ID          \n",
      "\t\t  ,s.SAMPLE_DATE                                          SAMPLE_Date   -- ,to_char(sf.sample_dm, 'DD/MM/YYYY HH24:MI:SS') SAMPLE_DT  \n",
      "\t\t  ,cast(CONVERT(VARCHAR(8),s.sample_Date, 112) as int) as SK_Sample_Date  \t\t  \t\t  \n",
      "         /*Parameter identifieres*/        \n",
      "          ,s.Parameter_Critical_Flag\t\t    \n",
      "          ,case when s.SPC_FLAG_EXTERN='Y' or s.SPC_FLAG_INTERN='Y'\n",
      "\t\t  then 'Y' else 'N' end as                                Is_Sample_Deleted_Flg   -- instead of Is_Batch_Deleted_Flag      \n",
      "         /* Sample identifires */                     \n",
      "         /*sample dates*/         \t\t \n",
      "          ,s.EXT_MV\t\t\t\t\t                        \t  SAMPLE_Mean          \n",
      "          ,s.EXT_SIGMA\t\t\t                         \t\t  SAMPLE_stdev\n",
      "          ,s.EXT_MIN\t\t\t\t\t\t\t\t\t\t\t  SAMPLE_Minimum\n",
      "          ,s.EXT_MAX\t\t\t\t\t\t\t\t\t\t\t  SAMPLE_Maximum\n",
      "          ,s.EXT_MEDIAN\t\t\t\t\t\t\t\t\t\t\t  SAMPLE_Median\n",
      "\t\t  ,s.SPEC_TARGET\t\t\t\t\t\t\t\t\t\t  Spec_target\n",
      "\t\t  ,s.EXT_SAMPLE_SIZE\t\t\t\t\t\t\t\t\t  SAMPLE_Size\n",
      "                \n",
      "         /*sample SPEC attr*/          \n",
      "          ,s.SPEC_LOWER\t\t\t\t\t\t\t\t\t\t\t  LSL        \n",
      "          ,s.SPEC_UPPER\t\t\t\t\t\t\t\t\t\t\t  USL\n",
      "\t\t  --, substring(s.[SPEC_LIMIT_ENABLE],1,1) as USL_enabled\n",
      "\t\t  , substring(s.[SPEC_LIMIT_ENABLE],2,1) as               SL_enabled --  originaly (LSL)\n",
      "          /*sample Control attr*/          \n",
      "          --,s.EXT_MV_LCL\t\t\t\t\t\t\t\t\t\t      LCL --SAMPLE_mean_lcl\n",
      "          --,s.EXT_MV_CENTER\t\t\t\t\t\t\t\t\t      CCL --SAMPLE_mean_center\n",
      "          --,s.EXT_MV_UCL\t\t\t\t\t\t\t\t\t\t      UCL --SAMPLE_mean_ucl\n",
      "          --,case when substring(s.[LIMIT_ENABLE],1,1)='Y' and substring(s.[LIMIT_ENABLE],10,1)= 'Y'\n",
      "\t\t  --then 'Y' else 'N' END as                                CL_enabled  --  originaly (LCL)\n",
      "\t\t  --,case when substring(s.[LIMIT_ENABLE],2,1) ='Y'and substring(s.[LIMIT_ENABLE],10,1)= 'Y' then 'Y' else 'N'  END as UCL_enabled\n",
      "\t\t  --,case when substring(s.[LIMIT_ENABLE],7,1) ='Y' and substring(s.[LIMIT_ENABLE],10,1)= 'Y' then 'Y' else 'N' END as CCL_enabled\t\t\n",
      "\t\t  ,s.CH_ID\t\t\n",
      "\t\t  ,ETL_DATE\n",
      "\t\t  --,Year(s.SAMPLE_DATE)*100+month(s.SAMPLE_DATE) as YearMonth\n",
      "         /*sample flags*/                    \n",
      "           -- ,s.VIOL_COUNT               SAMPLE_violation_count          \t\t  \n",
      "     FROM T_EXT_SAMPLES s\n",
      "\t  WHERE \n",
      "\t\t /* If channel has CKC_ID>0 then its CKC_ID=0 will be excluded*/\n",
      "\t\ts.SAMPLE_DATE >= s.START_PRD_DT and s.SAMPLE_DATE <= s.END_PRD_DT\n",
      "\t\t--AND SAMPLE_DATE < '2022-10-13 09:30:00'\n",
      "\t\t--AND SAMPLE_DATE < '2022-10-20 09:30:00'\n",
      "\n",
      "\n",
      "\n",
      " \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Yearmonth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [24], line 100\u001b[0m\n\u001b[0;32m     98\u001b[0m dynamic_sql \u001b[39m=\u001b[39m Loaddata(first_day_of_month,last_day_of_month,QueryPath)   \n\u001b[0;32m     99\u001b[0m \u001b[39mprint\u001b[39m(dynamic_sql)\n\u001b[1;32m--> 100\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mfinish dynmic\u001b[39m\u001b[39m'\u001b[39m,Yearmonth)\n\u001b[0;32m    101\u001b[0m Dataframe \u001b[39m=\u001b[39m pandas_read_df(dynamic_sql,sql_native_space)\n\u001b[0;32m    102\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mfinish READ QUERY\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Yearmonth' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta,date\n",
    "import calendar\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# Build Dynamic Query per yearmonth filter\n",
    "def Loaddata(first_day_of_month:datetime, last_day_of_month:datetime,querypath:str) -> str:\n",
    "    \"\"\" Get the Start date and the end date of the desired query range to filter\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    first_day_of_month : datetime\n",
    "        start date of main fact to filter\n",
    "    last_day_of_month : datetime\n",
    "        end date of main fact to filter\n",
    "    querypath: str\n",
    "    \n",
    "    log created\n",
    "    ----------\n",
    "    create dynamic_sql txt file to review the sql script \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str : query string the change dynmicly by parameters\n",
    "    \"\"\"\n",
    "    with open(querypath) as f: query_file = f.read()\n",
    "    sql_string = query_file.replace(\"{first_day_of_month}\",\"'\"+str(first_day_of_month)+\"'\")\\\n",
    "        .replace(\"{last_day_of_month}\",\"'\"+str(last_day_of_month)+\"'\").replace(\"{FolderPath}\",str( \"'\"+FolderPath+\"'\"))\n",
    "    dynamic_sql = f\"\"\"{sql_string}\"\"\"\n",
    "    file = open(\"dynamic_sql.txt\", \"w\")\n",
    "    a = file.write(dynamic_sql)\n",
    "    file.close()\n",
    "    return dynamic_sql\n",
    "\n",
    "def sprak_read_df(query,schema,conn):\n",
    "    \"\"\" Read Query from Source >  Return DATAFRAME per yearmonth filter\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    query : str\n",
    "        dynamic sql \n",
    "    schema : str\n",
    "        table schema\n",
    "    conn: connection to db\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe : return spark dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    startTime = time.time()\n",
    "    spark_read_df = spark.read.format('jdbc') \\\n",
    "    .option('url',conn) \\\n",
    "    .option('driver', 'com.microsoft.sqlserver.jdbc.SQLServerDriver')\\\n",
    "    .option('query', query) \\\n",
    "    .schema(schema) \\\n",
    "    .load()\n",
    "    executionTime = (time.time() - startTime)\n",
    "    return spark_read_df\n",
    "\n",
    "\n",
    "def pandas_read_df(dynamic_sql,sql_native_space):\n",
    "    fmt='%Y%m%d %H:%M:%S'             \n",
    "    pandas_read_pdf = pd.read_sql_query(sql = dynamic_sql,\\\n",
    "                                        con = sql_native_space,#\\\n",
    "                                        #parse_dates={'SAMPLE_Date':fmt,'ETL_DATE':fmt}\\\n",
    "                                       )                                  \n",
    "    return pandas_read_pdf\n",
    "\n",
    "def WriteToParqurtFile(Dataframe,Zone,Table_Folder,Year,Yearmonth):\n",
    "    spark_read_df=spark.createDataFrame(Dataframe,ProductionRunSchema) #ProductionRunSchema\n",
    "    spark_read_df.coalesce(1).write.parquet(LandingPath + Table_Folder +str(Year)+\"/\"+str(Yearmonth),mode=\"overwrite\") \n",
    "    executionTime = (time.time() - startTime)\n",
    "#\n",
    "\n",
    "def start_end_month(date:datetime):\n",
    "    \n",
    "    first_day = beg + relativedelta(day=1)\n",
    "    last_day = (beg.replace(day=1) + timedelta(days=32)).replace(day=1)\n",
    "    return first_day,last_day\n",
    "\n",
    "ProductList = {}\n",
    "ProductList['Gemini3'] = {'StartYearMonth': '2020-10-01', 'FullPath': 'New Era+AFL+WSM-> Gemini 3','FolderOutput':'Gemini', 'ProductName': 'GEMINI3'}\n",
    "ProductList['RotemSP'] = {'StartYearMonth': '2020-11-01', 'FullPath': 'New Era+AFL+WSM-> NewEra-SpeedUp-> Rotem SP','FolderOutput':'RotemSP', 'ProductName': 'ROTEM_SP'}\n",
    "ProductList['IrisPlusSP'] = {'StartYearMonth': '2020-10-01', 'FullPath': 'New Era+AFL+WSM-> NewEra-SpeedUp-> Iris Plus SP','FolderOutput':'IrisPlusSP', 'ProductName': 'IRIS PLUS_SP'}\n",
    "ProductList['Timna2'] = {'StartYearMonth': '2020-10-01', 'FullPath': 'New Era+AFL+WSM-> Timna 2','FolderOutput':'Timna2', 'ProductName': 'TIMNA 2'}\n",
    "\n",
    "\n",
    "### Production run queries variables\n",
    "## **************************************\n",
    "# find Min Date To load Data\n",
    "querypath = \"IngestQueries/ProductionLine.sql\"\n",
    "min_date = '2019-01-01'\n",
    "StartYear = \"2018\"\n",
    "FolderPath = ProductList['Timna2']['FullPath']\n",
    "ProductName = ProductList['Timna2']['ProductName']\n",
    "with open(querypath) as f: query_file = f.read()\n",
    "sql_string = query_file.replace(\"{StartYear}\",str(StartYear)).replace(\"{FolderPath}\",str(\"'\"+FolderPath+\"'\")).replace(\"{FolderPath}\",str( \"'\"+FolderPath+\"'\"))\n",
    "dynamic_sql = f\"\"\"{sql_string}\"\"\"\n",
    "#print(str(\"'\"+FolderPath+\"'\"))\n",
    "\n",
    "#min_query_date_pdf = pd.read_sql_query(sql = dynamic_sql,  con = sql_native_space)  \n",
    "#start_date = min_query_date_pdf['MinYearMonth'][0].strftime(\"%Y-%m-01\")\n",
    "#min_query_date = str(max(min_date,start_date))\n",
    "## **************************************\n",
    "\n",
    "min_query_date= '2022-12-01'   \n",
    "Zone = LandingPath\n",
    "product = \"IrisPlusSP\"   # Iris 6234  '2022-11-01'  # rptem Count before inner 2460\n",
    "Table_Folder = \"/ProductionRun/ProductionRun_hist/\"+product+\"/\"\n",
    "#Table_Folder = \"/New_Era/New_era_details/\"\n",
    "Table = \"ProductionRun\"\n",
    "QueryPath = \"QaQueries\\ProductionLine_test.sql\"\n",
    "#Test > \\QaQueries\\ProductionLine_test.sql   / prod \"IngestQueries/ProductionLine.sql\"\n",
    "startPipelineTime =  time.time()\n",
    "\n",
    "for beg in pd.date_range(min_query_date,'2022-12-31' , freq='MS'): # date.today() date.today().strftime(\"%Y-%m-01\")\n",
    "    first_day_of_month, last_day_of_month = start_end_month(beg)\n",
    "    # startTime = time.time()\n",
    "    # Yearmonth = beg.strftime(\"%Y%m\")\n",
    "    # Year=Yearmonth[0:4]                                     \n",
    "    dynamic_sql = Loaddata(first_day_of_month,last_day_of_month,QueryPath)   \n",
    "    print(dynamic_sql)\n",
    "    print('finish dynmic',Yearmonth)\n",
    "    Dataframe = pandas_read_df(dynamic_sql,sql_native_space)\n",
    "    print('finish READ QUERY')\n",
    "    Dataframe.to_csv(\"test3.csv\",index= False)\n",
    "    #print(Dataframe.columns)\n",
    "    WriteToParqurtFile(Dataframe,Zone,Table_Folder,Year,Yearmonth)\n",
    "    \n",
    "    executionTime = (time.time() - startTime)\n",
    "    print(f'Execution time in seconds for: Table '+Table+' > YearMonth: '+ Yearmonth +\"  is: \"+ str(round(executionTime,2))+ \"   numberofrow:  \" + str(len(Dataframe.index)))\n",
    "\n",
    "executionTime = (time.time() - startPipelineTime)\n",
    "print(f'{datetime.now()} +   total Execution time in seconds for: Table '+Table+' > all yearmonth is: '+ str(round(executionTime,2)))   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxx >   yyy\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'first_day_of_month' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m querypath \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mQaQueries\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mProductionLine_test copy.txt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(querypath) \u001b[39mas\u001b[39;00m f: query_file \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[1;32m---> 11\u001b[0m sql_string \u001b[39m=\u001b[39m query_file\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m{first_day_of_month}\u001b[39;00m\u001b[39m\"\u001b[39m,\u001b[39mstr\u001b[39m(first_day_of_month))\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m{last_day_of_month}\u001b[39;00m\u001b[39m\"\u001b[39m,\u001b[39mstr\u001b[39m(last_day_of_month))\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m{FolderPath}\u001b[39;00m\u001b[39m\"\u001b[39m,\u001b[39mstr\u001b[39m( \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mFolderPath\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m     12\u001b[0m \u001b[39mprint\u001b[39m(sql_string)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'first_day_of_month' is not defined"
     ]
    }
   ],
   "source": [
    "queryPath = \"\"\n",
    "#print(queryPath)\n",
    "query_file = \"\"\n",
    "#print(a2)\n",
    "sql_string = \"\"\n",
    "#print(a3)\n",
    "\n",
    "print(f\"xxxx > {sql_string}  yyy\")\n",
    "querypath = \"QaQueries\\ProductionLine_test copy.txt\"\n",
    "with open(querypath) as f: query_file = f.read()\n",
    "sql_string = query_file.replace(\"{first_day_of_month}\",str(first_day_of_month)).replace(\"{last_day_of_month}\",str(last_day_of_month)).replace(\"{FolderPath}\",str( \"'\"+FolderPath+\"'\"))\n",
    "print(sql_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\02-Data-Engineering\\\\pyspark\\\\02-LoadData'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-01 00:00:00\n",
      "2022-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import calendar\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "year =2022\n",
    "month = 12\n",
    "sample_date =  datetime(year, month, 1)\n",
    "first_day_of_month = sample_date + relativedelta(day=1)\n",
    "first_day_of_month = sample_date + relativedelta(day=31)\n",
    "print(first_day, last_day, sep='\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'socket' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [99], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(socket)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'socket' is not defined"
     ]
    }
   ],
   "source": [
    "print(socket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-01 00:00:00 2022-10-01 00:00:00\n",
      "2022-10-01 00:00:00 2022-11-01 00:00:00\n",
      "2022-11-01 00:00:00 2022-12-01 00:00:00\n",
      "2022-12-01 00:00:00 2023-01-01 00:00:00\n",
      "2023-01-01 00:00:00 2023-02-01 00:00:00\n",
      "2023-02-01 00:00:00 2023-03-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "min_query_date= '2022-09-01'   \n",
    "def start_end_month(date:datetime):\n",
    "    #print(beg)\n",
    "    # startTime = time.time()\n",
    "    \n",
    "    # Yearmonth = beg.strftime(\"%Y%m\")\n",
    "    # Year = beg.strftime(\"%Y\")\n",
    "    # Month = beg.strftime(\"%m\")\n",
    "    #sample_date =  datetime(int(Year), int(Month), 1)\n",
    "    first_day = beg + relativedelta(day=1)\n",
    "    last_day = (beg.replace(day=1) + datetime.timedelta(days=32)).replace(day=1)\n",
    "    return first_day,last_day\n",
    "for beg in pd.date_range(min_query_date,date.today() , freq='MS'): # date.today() date.today().strftime(\"%Y-%m-01\")\n",
    "    first_day_of_month, last_day_of_month = start_end_month(beg)\n",
    "    print(first_day, last_day)\n",
    "\n",
    "        #last_day = beg + relativedelta(month=1, day=1)\n",
    "        #last_day = first_day + relativedelta(month=1)\n",
    "    \n",
    "                           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(os.sep)\n",
    "# /\n",
    "\n",
    "print(os.sep is os.path.sep)\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2020', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2021', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2022', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2020\\\\202011', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2020\\\\202012', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2021\\\\202101', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2021\\\\202102', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2021\\\\202103', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2021\\\\202104', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2021\\\\202105', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2021\\\\202106', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2021\\\\202107', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2021\\\\202108', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2021\\\\202109', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2021\\\\202110', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2021\\\\202111', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2021\\\\202112', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2022\\\\202201', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2022\\\\202202', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2022\\\\202203', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2022\\\\202204', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2022\\\\202205', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2022\\\\202206', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2022\\\\202207', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2022\\\\202208', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2022\\\\202209', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2022\\\\202210', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2022\\\\202211', 'c:\\\\BI\\\\GitHub\\\\LearningSparkV2-master\\\\SuppliesML\\\\Data\\\\Dev\\\\Landing\\\\ProductionRun\\\\ProductionRun_hist\\\\IrisPlusSP\\\\2022\\\\202212']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from pathlib import Path, PurePosixPath\n",
    "# folder path\n",
    "#print(LandingPath) \n",
    "Table_Folder = \"ProductionRun/ProductionRun_hist/\"+product+\"/\"\n",
    "dir_path = os.path.join(LandingPath,Table_Folder.replace(f\"/\",\"\\\\\")) #, LandingPath\n",
    "    # path = Path(f\"data/{color}/{dataset_file}.parquet\")\n",
    "#print(dir_path)    \n",
    "    # df.to_parquet(path, compression=\"gzip\")\n",
    "    # path = PurePosixPath(\"data\",color,f\"{dataset_file}.parquet\") # gcp path\n",
    "# list to store files\n",
    "def fast_scandir(dirname):\n",
    "    subfolders= [f.path for f in os.scandir(dirname) if f.is_dir()]\n",
    "    for dirname in list(subfolders):\n",
    "        subfolders.extend(fast_scandir(dirname))\n",
    "    return subfolders\n",
    "\n",
    "# # Iterate directory\n",
    "# for path in os.listdir(dir_path):\n",
    "#     # check if current path is a file\n",
    "#     if os.path.isfile(os.path.join(dir_path, path)):\n",
    "#         res.append(path)\n",
    "print(list(fast_scandir(dir_path)))\n",
    "\n",
    "print(\"\\nWe are listing out only the directories in current directory -\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calendar import monthrange\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "firstdayofmonth = current.replace(day=1)\n",
    "endmonth = monthrange(current.year, current.month)\n",
    "lastdayofmonth = datetime(current.year, current.month, endmonth[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.1.2.1 read & write & Test Count / Show() all space BI yearmonth file to one parquet file\n",
    "Parameters_run_level_dfs = spark.read.format(\"parquet\").load(LandingPath + \"/ProductionRun/ProductionRun_hist/Timna2/2022/202201/*\") # try to read empty\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Product',\n",
       " 'Machine',\n",
       " 'FOLDER_PATH',\n",
       " 'Product_category',\n",
       " 'Product_eng_name',\n",
       " 'Series',\n",
       " 'Product_name_win',\n",
       " 'Size_Flag',\n",
       " 'Parameter_Name',\n",
       " 'Batch',\n",
       " 'SAMPLE_ID',\n",
       " 'SAMPLE_Date',\n",
       " 'SK_Sample_Date',\n",
       " 'Parameter_Critical_Flag',\n",
       " 'Is_Sample_Deleted_Flg',\n",
       " 'SAMPLE_Mean',\n",
       " 'SAMPLE_stdev',\n",
       " 'SAMPLE_Minimum',\n",
       " 'SAMPLE_Maximum',\n",
       " 'SAMPLE_Median',\n",
       " 'Spec_target',\n",
       " 'SAMPLE_Size',\n",
       " 'LSL',\n",
       " 'USL',\n",
       " 'SL_enabled',\n",
       " 'CH_ID',\n",
       " 'ETL_DATE']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parameters_run_level_dfs.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters_run_level_dfs = spark.read.format(\"parquet\").load(LandingPath + \"/ProductionRun/ProductionRun_hist/*/*/*\")\n",
    "Parameters_run_level_dfs = spark.read.format(\"parquet\").load(LandingPath + \"/ProductionRun/ProductionRun_hist/*/*/*/*\") # try to read empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parameters_run_level_dfs.createOrReplaceTempView(\"ld_Parameters_run_level_dfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_Parameters_run_level_dfs_new = spark.sql(\"select *, Year(SAMPLE_DATE)*100+Month(SAMPLE_DATE) As YearMonth from ld_Parameters_run_level_dfs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_Parameters_run_level_dfs_new.createOrReplaceTempView(\"ld_Parameters_run_level_dfs_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time in seconds: 6.96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "494670"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "ld_Parameters_run_level_dfs_new.coalesce(1).write.parquet(LandingPath + \"/ProductionRun/ProductionRun_Full/ProductionRun_Full_YM.parquet\",mode=\"overwrite\") #e\")#.saveAsTable(\"ld_blanket_lifespan_run_extended\")# = spark.read.format(\"parquet\").load(SourcePath + '/test')\n",
    "executionTime = (time.time() - startTime)\n",
    "print('Execution time in seconds: ' + str(round(executionTime,2)))\n",
    "Parameters_run_level_dfs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2.1 TEST Load from SpaceBI > production run jdbc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load With pyodbc - Blankets Lifespan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_read_sql_query\n",
      "after_convert_in_pandas\n",
      "c:\\BI\\GitHub\\LearningSparkV2-master\\SuppliesML\\Data\\Dev\\Landing\\Blanket_lifespan\\2022\\202212\\test\n",
      "Execution time in seconds for: Table Blanketslifespan > YearMonth: 202212  is: 0.32   numberofrow:  10\n",
      "2023-02-16 20:30:29.329376 total Execution time in seconds for: Table Blanketslifespan > all yearmonth is: 0.32\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta,date\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType, LongType\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "from pathlib import Path, PurePosixPath\n",
    "\n",
    "# Build Dynamic Query per yearmonth filter\n",
    "def Loaddata(YearMonth,StartYear,querypath):\n",
    "    \n",
    "    #YearMonth =\"201903\"\n",
    "    Year=YearMonth[0:4]\n",
    "    with open(querypath) as f: query_file = f.read()\n",
    "    sql_string = query_file.replace(\"{YearMonth}\",str(YearMonth)).replace(\"{StartYear}\",str(StartYear))\n",
    "    dynamic_sql = f\"\"\"{sql_string}\"\"\"\n",
    "    #print(dynamic_sql)\n",
    "    #print(YearMonth)\n",
    "    return dynamic_sql\n",
    "\n",
    "\n",
    "convert_dict = {#'Blanket_Impact_RowID': str,\n",
    "                'Optimized_Lifespan': int,\n",
    "                #'Replacement_DateTime': str\n",
    "                }\n",
    " \n",
    "\n",
    "\n",
    "# Read Query from Source >  Return DATAFRAME per yearmonth filter\n",
    "def pandas_read_df(query,conn):   \n",
    "    startTime = time.time()\n",
    "    Pareto_pdf = pd.read_sql_query(sql = query, con = conn)     \n",
    "    print(\"after_read_sql_query\")\n",
    "    \n",
    "    Pareto_pdf = Pareto_pdf.astype(convert_dict)\n",
    "    #Pareto_pdf = Pareto_pdf.tz_localize(tz=\"Asia/Jerusalem\", nonexistent=pd.Timedelta('1H'))\n",
    "    Pareto_pdf['ETL_Date'] = pd.to_datetime(Pareto_pdf['ETL_Date']) \\\n",
    ".dt.tz_localize('Asia/Jerusalem',nonexistent=pd.Timedelta('1H'))\n",
    "    print(\"after_convert_in_pandas\")\n",
    "    Dataframe.to_csv(\"test4.csv\",index= False)\n",
    "    return Pareto_pdf\n",
    "\n",
    "\n",
    "def WriteToParqurtFile(Dataframe,Zone,Table_Folder,Year,YearMonth):\n",
    "    #Dataframe = Dateframe['Blanket_Impact_RowID'].astype('int')\n",
    "    spark_read_df_temp =spark.createDataFrame(Dataframe,Blanket_lifespan_installed_base_Schema)\n",
    "    #print(LandingPath)\n",
    "\n",
    "    #New_era_details_dfs.coalesce(2).write.parquet(LandingPath + \"/New_Era/New_Era_Run_Summary_N\",mode=\"overwrite\")\n",
    "    \n",
    "  \n",
    "    path = os.path.join(LandingPath,Table_Folder,str(Year),str(YearMonth),\"x\")\n",
    "    #path = PurePosixPath(\"c:/BI/GitHub/LearningSparkV2-master/SuppliesML/Data/Dev/Landing/Blanket_lifespan/2022/202212/c.parquet\")\n",
    "    # c:\\BI\\GitHub\\LearningSparkV2-master\\SuppliesML\\Data\\Dev\\Landing\\Blanket_lifespan\\2022\\202212\\\n",
    "    print(path)\n",
    "    spark_read_df_temp.write.format(\"parquet\")\\\n",
    "                .mode(\"overwrite\")\\\n",
    "                .save(\"v.parquet\")\n",
    "    #spark_read_df_temp.coalesce(8).write.parquet(path=path,mode=\"overwrite\") \n",
    "    executionTime = (time.time() - startTime)\n",
    "\n",
    "    \n",
    "Zone = LandingPath\n",
    "#Table_Folder = \"/Blanket_lifespan/Blanket_lifespan_Hist_New/\"\n",
    "Table_Folder = \"Blanket_lifespan\"\n",
    "Table = \"Blanketslifespan\"\n",
    "QueryPath = \"IngestQueries/Blanketslifespan.sql\"\n",
    "#Conn = odbc_Hubble_ITG\n",
    "StartYear = \"2019\"\n",
    "startPipelineTime =  time.time()\n",
    "for beg in pd.date_range('2022-12-01','2022-12-06', freq='MS'): # date.today()\n",
    "    startTime = time.time()\n",
    "    YearMonth = beg.strftime(\"%Y%m\")\n",
    "    Year=YearMonth[0:4]                                     \n",
    "    dynamic_sql = Loaddata(YearMonth,StartYear,QueryPath)  \n",
    "    #print(dynamic_sql)\n",
    "    Dataframe = pandas_read_df(dynamic_sql,odbc_Hubble_ITG  ) #sql_native_Hubble_ITG\n",
    "    path = os.path.join(LandingPath,Table_Folder,str(Year),str(YearMonth),\"test\")\n",
    "    print(path)\n",
    "    Dataframe.to_parquet(f\"{path}.parquet\")\n",
    "    #os.makedirs(os.path.dirname(LandingPath + Table_Folder +\"/\"+str(YearMonth)+\"/\"), exist_ok=True)\n",
    "    #path = os.path.join(LandingPath,Table_Folder,str(Year),str(YearMonth),\"\")\n",
    "    #Dataframe.to_parquet(f\"{path}\")#LandingPath + Table_Folder +\"/\"+str(YearMonth)+\"/\")\n",
    "    #WriteToParqurtFile(Dataframe,Zone,Table_Folder,Year,YearMonth)\n",
    "    executionTime = (time.time() - startTime)\n",
    "    print(f'Execution time in seconds for: Table '+Table+' > YearMonth: '+ YearMonth +\"  is: \"+ str(round(executionTime,2))+ \"   numberofrow:  \" + str(len(Dataframe.index)))\n",
    "    \n",
    "executionTime = (time.time() - startPipelineTime)\n",
    "print(f'{datetime.now()} total Execution time in seconds for: Table '+Table+' > all yearmonth is: '+ str(round(executionTime,2))) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\BI\\GitHub\\LearningSparkV2-master\\SuppliesML\\Data\\Dev\\Landing\\Blanket_lifespan\\2022\\202212\\\n",
      "\\\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "Table_Folder = \"Blanket_lifespan\"\n",
    "path = os.path.join(LandingPath,Table_Folder,str(Year),str(YearMonth),\"\")\n",
    "print(path)\n",
    "\n",
    "print(os.sep)\n",
    "# /\n",
    "\n",
    "print(os.sep is os.path.sep)\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "Blanketslifespan = spark.read.format(\"parquet\").load(LandingPath + Table_Folder +\"/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "Blanketslifespan.coalesce(1).write.parquet(LandingPath + \"/Blanket_lifespan/Blanket_lifespan_hist_PM.parquet\",mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1727554795370864,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "1-LoadTxt2Parquet Test ETL",
   "notebookOrigID": 936863364878517,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
